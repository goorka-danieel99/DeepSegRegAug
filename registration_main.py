# -*- coding: utf-8 -*-
"""registration_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12vK2ALWWKBHBw8z8pwizi92B_y4OF6z0
"""

!pip install simpleitk

import numpy as np
import SimpleITK as sitk
import torch
import pandas as pd
from tqdm import tqdm
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
import torch.utils.tensorboard as tb
import torch

from dataloaders import RegLoader, ValLoader
from models import RegModel
from cost_functions import diffusion, mind_loss
from evaluation import hd95, dc
from config import reg_params

# Loading data
train_data = pd.read_csv(reg_params['train_path'])
val_data = pd.read_csv(reg_params['val_path'])


train_images = reg_params['path'] + train_data['train_images'].astype(str)                                     
train_labels = reg_params['path'] + train_data['train_masks'].astype(str)   

val_images = reg_params['path'] + val_data['val_images'].astype(str)   
val_labels = reg_params['path'] + val_data['val_masks'].astype(str)

# Pairing images and labels
fixed_images = train_images
moving_images = train_images

img_pairs = []

for fixed in fixed_images:
  for moving in moving_images:
    if fixed != moving:
      if [moving, fixed] not in img_pairs:
        img_pairs.append([fixed, moving])




val_fixed_images = val_images
val_moving_images = val_images

val_img_pairs = []

for fixed in val_fixed_images:
  for moving in val_moving_images:
    if fixed != moving:
      if [moving, fixed] not in val_img_pairs:
        val_img_pairs.append([fixed, moving])

val_fixed_labels = val_labels
val_moving_labels = val_labels

val_labels_pairs = []

for fixed in val_fixed_labels:
  for moving in val_moving_labels:
    if fixed != moving:
      if [moving, fixed] not in val_labels_pairs:
        val_labels_pairs.append([fixed, moving])

train_dataset = RegLoader(img_pairs)
val_dataset = ValLoader(val_img_pairs, val_labels_pairs)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=reg_params['train_batch_size'], shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=reg_params['val_batch_size'], shuffle=True)

def df_field_v2(tensor: torch.Tensor, displacement_field: torch.Tensor, grid: torch.Tensor=None, device: str="cuda:0", mode: str='bilinear'):
  """
  Courtesy of: dr Marek Wodziński

  """
  if grid is None:
    grid = generate_grid(tensor.size(), device=device)

    sampling_grid = grid + displacement_field
    transformed_tensor = F.grid_sample(tensor, sampling_grid, mode=mode, padding_mode='zeros', align_corners=False)
    return transformed_tensor

def df_field_v2_eval(tensor: torch.Tensor, displacement_field: torch.Tensor, grid: torch.Tensor=None, device: str="cuda:0", mode: str='nearest'):
  """
  Courtesy of: dr Marek Wodziński

  """
  if grid is None:
    grid = generate_grid(tensor.size(), device=device)

    sampling_grid = grid + displacement_field
    transformed_tensor = F.grid_sample(tensor, sampling_grid, mode=mode, padding_mode='zeros', align_corners=False)
    return transformed_tensor

def generate_grid(tensor_size: torch.Tensor, device="cuda:0"):
  """
  Courtesy of: dr Marek Wodziński

  """
  identity_transform = torch.eye(len(tensor_size)-1, device=device)[:-1, :].unsqueeze(0)
  identity_transform = torch.repeat_interleave(identity_transform, tensor_size[0], dim=0)
  grid = F.affine_grid(identity_transform, tensor_size, align_corners=False)
  return grid

def count(model, dataloader, function):
  tab1 = []
  tab2 = []
  for fixed_image, moving_image, fixed_mask1, moving_mask1, fixed_mask2, moving_mask2 in tqdm(dataloader):
    fixed_image = fixed_image.to(device)
    moving_image = moving_image.to(device)
    moving_mask1 = moving_mask1.to(device)
    moving_mask2 = moving_mask2.to(device)

    ddf = model(torch.cat((moving_image, fixed_image), dim=1)).permute(0,2,3,4,1)
    pred_mask1 = df_field_v2_eval(moving_mask1, ddf)
    pred_mask2 = df_field_v2_eval(moving_mask2, ddf)

    fixed_mask1 = fixed_mask1.squeeze(0).squeeze(0)
    pred_mask1 = pred_mask1.squeeze(0).squeeze(0)
    fixed_mask1 = fixed_mask1.detach().to("cpu").numpy()
    pred_mask1 = pred_mask1.detach().to("cpu").numpy()

    fixed_mask2 = fixed_mask2.squeeze(0).squeeze(0)
    pred_mask2 = pred_mask2.squeeze(0).squeeze(0)
    fixed_mask2 = fixed_mask2.detach().to("cpu").numpy()
    pred_mask2 = pred_mask2.detach().to("cpu").numpy()

    pred_mask1[pred_mask1 > 0.5] = 1
    pred_mask1[pred_mask1 <= 0.5] = 0
    pred_mask2[pred_mask2 > 0.5] = 1
    pred_mask2[pred_mask2 <= 0.5] = 0

    tab1.append(function(fixed_mask1, pred_mask1))
    tab2.append(function(fixed_mask2, pred_mask2))

  return tab1, tab2

# Train loop

runs = reg_params['runs']
writer = tb.SummaryWriter(str(runs) + '/reg', flush_secs=1)


model = RegModel()
device = torch.device("cuda:0")
model = model.to(device)

optimizer = torch.optim.Adam(model.parameters())

scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.995**epoch) 

training_size = len(train_loader.dataset)
val_size = len(val_loader.dataset)

epochs = reg_params['num_epochs']

loss_history = []
regu_history = []
mind_history = []
val_loss_history = []
val_regu_history = []
val_mind_history = []



for epoch in range(epochs):
  train_loss = 0
  r_loss = 0
  m_loss = 0
  val_loss = 0
  r_loss_val = 0
  m_loss_val = 0
  for fixed_image, moving_image in tqdm(train_loader):

    fixed_image = fixed_image.to(device)
    moving_image = moving_image.to(device)

    optimizer.zero_grad()

    ddf = model(torch.cat((moving_image, fixed_image), dim=1)).permute(0,2,3,4,1)     
  
    pred_image = df_field_v2(moving_image, ddf)

    lambda_reg = reg_params['lambda_value']

    first = fixed_image.shape[0]

    mind = mind_loss(pred_image, fixed_image)
    regu_loss = diffusion(ddf)
    loss = mind + lambda_reg*regu_loss

    m_loss += first*mind.item()
    r_loss += first*regu_loss.item()

    train_loss += first*loss.item()

    loss.backward()
    optimizer.step()
  
  scheduler.step()

  train_loss /= training_size
  m_loss /= training_size
  r_loss /= training_size

  print('\nTrain Loss', train_loss)
  print('Train Mind Loss', m_loss)
  print('Train Regu Loss', r_loss)
  loss_history.append(train_loss)
  regu_history.append(r_loss)
  mind_history.append(m_loss)

  writer.add_scalars(f'check_info mind train', {
              'Train Mind Loss': m_loss,
              }, global_step = epoch)

  writer.add_scalars(f'check_info regu train', {
              'Train Regu Loss': r_loss,
              }, global_step = epoch)

  writer.add_scalars(f'check_info train loss', {
              'Train Loss': train_loss,
              }, global_step = epoch)

  for fixed_image, moving_image, fixed_mask1, moving_mask1, fixed_mask2, moving_mask2 in tqdm(val_loader):
    fixed_image = fixed_image.to(device)
    moving_image = moving_image.to(device)
  
    ddf = model(torch.cat((moving_image, fixed_image), dim=1)).permute(0,2,3,4,1)     
  
    pred_image = df_field_v2(moving_image, ddf)

    lambda_reg = lambda_value

    first = fixed_image.shape[0]

    mind = mind_loss(pred_image, fixed_image)
    regu_loss = diffusion(ddf)
    loss = mind + lambda_reg*regu_loss

    m_loss_val += first*mind.item()
    r_loss_val += first*regu_loss.item()

    val_loss += first*loss.item()

  val_loss /= val_size
  m_loss_val /= val_size
  r_loss_val /= val_size

  print('\nVal Loss', val_loss)
  print('Val Mind Loss', m_loss_val)
  print('Val Regu Loss', r_loss_val)
  val_loss_history.append(val_loss)
  val_regu_history.append(r_loss_val)
  val_mind_history.append(m_loss_val)

  writer.add_scalars(f'reg/check_info mind val', {
              'Val Mind Loss': m_loss_val,
              }, global_step = epoch)

  writer.add_scalars(f'reg/check_info regu val', {
              'Val Regu Loss': r_loss_val,
              }, global_step = epoch)

  writer.add_scalars(f'reg/check_info val loss', {
              'Val Loss': val_loss,
              }, global_step = epoch)

# Conting dc every 10 epochs
  if (epoch+1)%10 == 0:
    model.eval()
    with torch.no_grad():

      dc1, dc2 = count(model, val_loader, dc)
      av1 = sum(dc1)/len(dc1)
      av2 = sum(dc2)/len(dc2)

    model.train()
    print('\nDC after ' + str(epoch) + ' epochs: ' + str(av1))
    print('DC after ' + str(epoch) + ' epochs: ' + str(av2))

  
# Saving model
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_loss': loss_history,
    'regu_train_loss': regu_history,
    'mind_train_loss': mind_history,
    'val_loss': val_loss_history,
    'regu_val_loss': val_regu_history,
    'mind_val_loss': val_mind_history,
}, reg_params['reg_model_path'])