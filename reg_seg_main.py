# -*- coding: utf-8 -*-
"""reg_seg_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HHTuGSpHgJlQTBat7gF1Bn_HJP4yaQCk
"""

!pip install simpleitk

import numpy as np
import SimpleITK as sitk
import torch
import pandas as pd
from tqdm import tqdm
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
import torch.utils.tensorboard as tb
import torch

from dataloaders import RegSegLoader, ValLoader
from models import RegSegModel
from cost_functions import diffusion, mind_loss
from evaluation import hd95, dc
from config import regseg_params

# Loading data
train_data = pd.read_csv(regseg_params['train_path'])
val_data = pd.read_csv(regseg_params['val_path'])


train_images = regseg_params['path'] + train_data['train_images'].astype(str)                                     
train_labels = regseg_params['path'] + train_data['train_masks'].astype(str)   

val_images = regseg_params['path'] + val_data['val_images'].astype(str)   
val_labels = regseg_params['path'] + val_data['val_masks'].astype(str)

# Pairing images and labels
fixed_images = train_images
moving_images = train_images

img_pairs = []

for fixed in fixed_images:
  for moving in moving_images:
    if fixed != moving:
      if [moving, fixed] not in img_pairs:
        img_pairs.append([fixed, moving])

fixed_labels = train_labels
moving_labels = train_labels

labels_pairs = []

for fixed in fixed_labels:
  for moving in moving_labels:
    if fixed != moving:
      if [moving, fixed] not in labels_pairs:
        labels_pairs.append([fixed, moving])



val_fixed_images = val_images
val_moving_images = val_images

val_img_pairs = []

for fixed in val_fixed_images:
  for moving in val_moving_images:
    if fixed != moving:
      if [moving, fixed] not in val_img_pairs:
        val_img_pairs.append([fixed, moving])

val_fixed_labels = val_labels
val_moving_labels = val_labels

val_labels_pairs = []

for fixed in val_fixed_labels:
  for moving in val_moving_labels:
    if fixed != moving:
      if [moving, fixed] not in val_labels_pairs:
        val_labels_pairs.append([fixed, moving])

train_dataset = RegSegLoader(img_pairs, labels_pairs)
val_dataset = ValLoader(val_img_pairs, val_labels_pairs)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=regseg_params['train_batch_size'], shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=regseg_params['val_batch_size'], shuffle=True)

def df_field_v2(tensor: torch.Tensor, displacement_field: torch.Tensor, grid: torch.Tensor=None, device: str="cuda:0", mode: str='bilinear'):
  """
  Courtesy of: dr Marek Wodziński

  """
  if grid is None:
    grid = generate_grid(tensor.size(), device=device)

    sampling_grid = grid + displacement_field
    transformed_tensor = F.grid_sample(tensor, sampling_grid, mode=mode, padding_mode='zeros', align_corners=False)
    return transformed_tensor

def df_field_v2_eval(tensor: torch.Tensor, displacement_field: torch.Tensor, grid: torch.Tensor=None, device: str="cuda:0", mode: str='nearest'):
  """
  Courtesy of: dr Marek Wodziński

  """
  if grid is None:
    grid = generate_grid(tensor.size(), device=device)

    sampling_grid = grid + displacement_field
    transformed_tensor = F.grid_sample(tensor, sampling_grid, mode=mode, padding_mode='zeros', align_corners=False)
    return transformed_tensor

def generate_grid(tensor_size: torch.Tensor, device="cuda:0"):
  """
  Courtesy of: dr Marek Wodziński

  """
  identity_transform = torch.eye(len(tensor_size)-1, device=device)[:-1, :].unsqueeze(0)
  identity_transform = torch.repeat_interleave(identity_transform, tensor_size[0], dim=0)
  grid = F.affine_grid(identity_transform, tensor_size, align_corners=False)
  return grid

def count(model, dataloader, function):
  seg_f1 = []
  seg_f2 = []
  seg_m1 = []
  seg_m2 = []

  reg_1 = []
  reg_2 = []

  for fixed_image, moving_image, fixed_mask1, moving_mask1, fixed_mask2, moving_mask2 in tqdm(dataloader):

    fixed_image = fixed_image.to(device)
    moving_image = moving_image.to(device)
    
    moving_mask1 = moving_mask1.to(device)
    moving_mask2 = moving_mask2.to(device)
    fixed_mask1 = fixed_mask1.to(device)
    fixed_mask2 = fixed_mask2.to(device)
    
    seg_pred, reg_pred =  model(moving_image, fixed_image)

    #registration

    pred_mask1 = df_field_v2_eval(moving_mask1, reg_pred)
    pred_mask2 = df_field_v2_eval(moving_mask2, reg_pred)


    fixed_mask1 = fixed_mask1.squeeze(0).squeeze(0)
    pred_mask1 = pred_mask1.squeeze(0).squeeze(0)
    fixed_mask1 = fixed_mask1.detach().to("cpu").numpy()
    pred_mask1 = pred_mask1.detach().to("cpu").numpy()

    fixed_mask2 = fixed_mask2.squeeze(0).squeeze(0)
    pred_mask2 = pred_mask2.squeeze(0).squeeze(0)
    fixed_mask2 = fixed_mask2.detach().to("cpu").numpy()
    pred_mask2 = pred_mask2.detach().to("cpu").numpy()

    pred_mask1[pred_mask1 > 0.5] = 1
    pred_mask1[pred_mask1 <= 0.5] = 0
    pred_mask2[pred_mask2 > 0.5] = 1
    pred_mask2[pred_mask2 <= 0.5] = 0

    reg_1.append(function(fixed_mask1, pred_mask1))
    reg_2.append(function(fixed_mask2, pred_mask2))


    #segmentation
    seg_pred = seg_pred.squeeze(0)

    #-----fixed--------
    seg_pred_f1 = seg_pred[0,:,:,:]
    seg_pred_f2 = seg_pred[2,:,:,:]

    seg_pred_f1 = seg_pred_f1.detach().to("cpu").numpy()

    seg_pred_f2 = seg_pred_f2.detach().to("cpu").numpy()

    seg_pred_f1[seg_pred_f1 > 0.5] = 1
    seg_pred_f1[seg_pred_f1 <= 0.5] = 0
    seg_pred_f2[seg_pred_f2 > 0.5] = 1
    seg_pred_f2[seg_pred_f2 <= 0.5] = 0

    seg_f1.append(function(seg_pred_f1, fixed_mask1))
    seg_f2.append(function(seg_pred_f2, fixed_mask2))
    
    #-----moving--------
    seg_pred_m1 = seg_pred[1,:,:,:]
    seg_pred_m2 = seg_pred[3,:,:,:]

    moving_mask1 = moving_mask1.squeeze(0).squeeze(0)
    moving_mask2 = moving_mask2.squeeze(0).squeeze(0)

    seg_pred_m1 = seg_pred_m1.detach().to("cpu").numpy()
    moving_mask1 = moving_mask1.detach().to("cpu").numpy()

    seg_pred_m2 = seg_pred_m2.detach().to("cpu").numpy()
    moving_mask2 = moving_mask2.detach().to("cpu").numpy()

    seg_pred_m1[seg_pred_m1 > 0.5] = 1
    seg_pred_m1[seg_pred_m1 <= 0.5] = 0
    seg_pred_m2[seg_pred_m2 > 0.5] = 1
    seg_pred_m2[seg_pred_m2 <= 0.5] = 0

    seg_m1.append(function(seg_pred_m1, moving_mask1))
    seg_m2.append(function(seg_pred_m2, moving_mask2))

  return seg_f1, seg_f2, seg_m1, seg_m2, reg_1, reg_2

# Train Loop
runs = regseg_params['runs']
writer = tb.SummaryWriter(str(runs) + '/regseg', flush_secs=1)

model = RegSegModel()
device = torch.device('cuda:0')
model = model.to(device)

seg_criterion = nn.BCEWithLogitsLoss()

optimizer = torch.optim.Adam(model.parameters())

epochs = regseg_params['num_epochs']

training_size = len(train_loader.dataset)
val_size = len(val_loader.dataset)

seg_loss_history = []
reg_loss_history = []
regu_history = []
mind_history = []
val_seg_loss_history = []
val_reg_loss_history = []
val_regu_history = []
val_mind_history = []


for epoch in range(epochs):

  seg_running_loss = 0
  reg_running_loss = 0
  r_loss = 0
  m_loss = 0
  val_seg_running_loss = 0
  val_reg_running_loss = 0
  r_loss_val = 0
  m_loss_val = 0
  step = 0

  for fixed_image, moving_image, fixed_mask1, moving_mask1, fixed_mask2, moving_mask2 in tqdm(train_loader):

    fixed_image = fixed_image.to(device)
    moving_image = moving_image.to(device)
    
    moving_mask1 = moving_mask1.to(device)
    moving_mask2 = moving_mask2.to(device)
    fixed_mask1 = fixed_mask1.to(device)
    fixed_mask2 = fixed_mask2.to(device)

    optimizer.zero_grad()

    seg_prediction, reg_prediction = model(moving_image, fixed_image)

    #-------segmentation-----------

    seg_loss_f1 = seg_criterion(seg_prediction[:, 0, :, :, :].unsqueeze(1), fixed_mask1)
    seg_loss_f2 = seg_criterion(seg_prediction[:, 2, :, :, :].unsqueeze(1), fixed_mask2)
    seg_loss_fixed = seg_loss_f1 + seg_loss_f2

    seg_loss_m1 = seg_criterion(seg_prediction[:, 1, :, :, :].unsqueeze(1), moving_mask1)
    seg_loss_m2 = seg_criterion(seg_prediction[:, 3, :, :, :].unsqueeze(1), moving_mask2)
    seg_loss_moving = seg_loss_m1 + seg_loss_m2 

    seg_loss = seg_loss_fixed + seg_loss_moving

    #-------registration-----------

    lambda_reg = regseg_params['lambda_value']

    first = fixed_image.shape[0]

    pred_image = df_field_v2(moving_image, reg_prediction)

    mind = mind_loss(pred_image, fixed_image)
    regu_loss = diffusion(reg_prediction)
    reg_loss = mind + lambda_reg*regu_loss    
    #-----------------------------
    loss = seg_loss + reg_loss

    loss.backward()
    optimizer.step()

    m_loss += first*mind.item()
    r_loss += first*regu_loss.item()

    seg_running_loss += first*seg_loss.item()

    reg_running_loss += first*reg_loss.item()


  seg_running_loss /= training_size
  reg_running_loss /= training_size
  m_loss /= training_size
  r_loss /= training_size


  print('\nTrain Segmentation Loss', seg_running_loss)
  print('Train Registration Loss', reg_running_loss)
  print('Train Mind Loss', m_loss)
  print('Train Regu Loss', r_loss)

  reg_loss_history.append(reg_running_loss)
  seg_loss_history.append(seg_running_loss)
  regu_history.append(r_loss)
  mind_history.append(m_loss)
  
  writer.add_scalars(f'check_info train', {
            'Segmentation': seg_running_loss,
            'Registration': reg_running_loss,
            }, global_step = epoch)

  writer.add_scalars(f'check_info regu train', {
            'Train Regu Loss': r_loss,
            }, global_step = epoch)

  writer.add_scalars(f'check_info mind train', {
            'Train Mind Loss': m_loss,
            }, global_step = epoch)



  for fixed_image, moving_image, fixed_mask1, moving_mask1, fixed_mask2, moving_mask2 in tqdm(val_loader):

    fixed_image = fixed_image.to(device)
    moving_image = moving_image.to(device)
    
    moving_mask1 = moving_mask1.to(device)
    moving_mask2 = moving_mask2.to(device)
    fixed_mask1 = fixed_mask1.to(device)
    fixed_mask2 = fixed_mask2.to(device)

    seg_prediction, reg_prediction = model(moving_image, fixed_image)

    #-------segmentation-----------

    seg_loss_f1 = seg_criterion(seg_prediction[:, 0, :, :, :].unsqueeze(1), fixed_mask1)
    seg_loss_f2 = seg_criterion(seg_prediction[:, 2, :, :, :].unsqueeze(1), fixed_mask2)
    seg_loss_fixed = seg_loss_f1 + seg_loss_f2

    seg_loss_m1 = seg_criterion(seg_prediction[:, 1, :, :, :].unsqueeze(1), moving_mask1)
    seg_loss_m2 = seg_criterion(seg_prediction[:, 3, :, :, :].unsqueeze(1), moving_mask2)
    seg_loss_moving = seg_loss_m1 + seg_loss_m2 

    seg_loss = seg_loss_fixed + seg_loss_moving

    #-------registration-----------

    lambda_reg = regseg_params['lambda_value']

    first = fixed_image.shape[0]

    pred_image = df_field_v2(moving_image, reg_prediction)

    mind = mind_loss(pred_image, fixed_image)
    regu_loss = diffusion(reg_prediction)
    reg_loss = mind + lambda_reg*regu_loss    
    #-----------------------------
    loss = seg_loss + reg_loss

    m_loss_val += first*mind.item()
    r_loss_val += first*regu_loss.item()

    val_seg_running_loss += first*seg_loss.item()

    val_reg_running_loss += first*reg_loss.item()

  val_seg_running_loss /= val_size
  val_reg_running_loss /= val_size
  m_loss_val /= val_size
  r_loss_val /= val_size


  print('Segmentation Loss val', val_seg_running_loss)
  print('Registration Loss val', val_reg_running_loss)
  print('Val Mind Loss', m_loss_val)
  print('Val Regu Loss', r_loss_val)

  val_reg_loss_history.append(val_reg_running_loss)
  val_seg_loss_history.append(val_seg_running_loss)
  val_regu_history.append(r_loss_val)
  val_mind_history.append(m_loss_val)
  
  writer.add_scalars(f'check_info val', {
            'Segmentation': val_seg_running_loss,
            'Registration': val_reg_running_loss,
            }, global_step = epoch)
  
  writer.add_scalars(f'reg/check_info regu val', {
              'Val Regu Loss': r_loss_val,
              }, global_step = epoch)

  writer.add_scalars(f'reg/check_info val loss', {
              'Val Loss': val_loss,
              }, global_step = epoch)
  
# Conting dc every 10 epochs
  if (epoch+1)%10 == 0:

    model.eval()
    with torch.no_grad():
      dc_s1, dc_s2, dc_t1, dc_t2, dc_r1, dc_r2 = count(model, val_loader, dc)
      av_s1 = sum(dc_s1)/len(dc_s1) #seg_source1
      av_s2 = sum(dc_s2)/len(dc_s2) #seg_source2
      av_t1 = sum(dc_t1)/len(dc_t1) #seg_target1
      av_t2 = sum(dc_t2)/len(dc_t2) #seg_target2
      av_r1 = sum(dc_r1)/len(dc_r1) #reg1
      av_r2 = sum(dc_r2)/len(dc_r2) #reg2

    print('\nDC seg fixed1 after ' + str(epoch) + ' epochs ' + str(av_s1))
    print('DC seg fixed2 after ' + str(epoch) + ' epochs: ' + str(av_s2))

    print('\nDC seg moving1 after ' + str(epoch) + ' epochs: ' + str(av_t1))
    print('DC seg moving2 after ' + str(epoch) + ' epochs: ' + str(av_t2))

    print('\nDC reg1 after ' + str(epoch) + ' epochs: ' + str(av_r1))
    print('DC reg2 after ' + str(epoch) + ' epochs: ' + str(av_r2))


torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'train_seg_loss': seg_loss_history,
    'train_reg_loss': reg_loss_history,
    'regu_train_loss': regu_history,
    'mind_train_loss': mind_history,
    'val_seg_loss': val_seg_loss_history,
    'val_reg_loss': val_reg_loss_history,
    'regu_val_loss': val_regu_history,
    'mind_val_loss': val_mind_history,
}, regseg_params['regseg_model_path'])