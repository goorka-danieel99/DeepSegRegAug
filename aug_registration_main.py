# -*- coding: utf-8 -*-
"""aug_registration_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qu3k9ITsRkRWiqwjq8LksYqv-PKpDWeM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import SimpleITK as sitk
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchio as tio
from torch.utils.data import DataLoader
from tqdm import tqdm

from transforms import transforms_dict
from config import config, model_params
from models import RegModel
from aug_dataloaders import train_list_reg, val_list_reg

# Create train and val tio Subjects Datasets
train_dataset = tio.SubjectsDataset(train_list_reg, transform=transforms_dict['t_nr'])
val_dataset = tio.SubjectsDataset(val_list_reg, transform=transforms_dict['val'])

# Create train and val Datalaoders
training_loader = DataLoader(train_dataset, batch_size=config['reg_batch_size'], shuffle=True, num_workers=2)
val_loader = DataLoader(val_dataset, batch_size=config['val_batch_size'], shuffle=True)

training_size = len(training_loader.dataset)
val_size = len(val_loader.dataset)


def df_field_v2(tensor: torch.Tensor, displacement_field: torch.Tensor, grid: torch.Tensor=None, device: str="cuda:0", mode: str='bilinear'):
  """
  Courtesy of: dr Marek Wodziński
  """
  if grid is None:
        grid = generate_grid(tensor.size(), device=device)

    sampling_grid = grid + displacement_field
    transformed_tensor = F.grid_sample(tensor, sampling_grid, mode=mode, padding_mode='zeros', align_corners=False)
    return transformed_tensor

    
def df_field_v2_eval(tensor: torch.Tensor, displacement_field: torch.Tensor, grid: torch.Tensor=None, device: str="cuda:0", mode: str='nearest'):
  """
  Courtesy of: dr Marek Wodziński
  """
  if grid is None:
        grid = generate_grid(tensor.size(), device=device)

    sampling_grid = grid + displacement_field
    transformed_tensor = F.grid_sample(tensor, sampling_grid, mode=mode, padding_mode='zeros', align_corners=False)
    return transformed_tensor

  
def generate_grid(tensor_size: torch.Tensor, device="cuda:0"):
  """
  Courtesy of: dr Marek Wodziński
  """
  identity_transform = torch.eye(len(tensor_size)-1, device=device)[:-1, :].unsqueeze(0)
  identity_transform = torch.repeat_interleave(identity_transform, tensor_size[0], dim=0)
  grid = F.affine_grid(identity_transform, tensor_size, align_corners=False)
  return grid

def count(model, dataloader, function):
  tab1 = []
  tab2 = []
  for batch in tqdm(dataloader):

    fixed_image = batch['imagef'][tio.DATA].to(device)
    fixed_mask1 = batch['maskf1'][tio.DATA].to(device)
    fixed_mask2 = batch['maskf2'][tio.DATA].to(device)

    moving_image = batch['imagem'][tio.DATA].to(device)
    moving_mask1 = batch['maskm1'][tio.DATA].to(device)
    moving_mask2 = batch['maskm2'][tio.DATA].to(device)


    ddf = model(torch.cat((moving_image, fixed_image), dim=1)).permute(0,2,3,4,1)
    pred_mask1 = df_field_v2_eval(moving_mask1, ddf)
    pred_mask2 = df_field_v2_eval(moving_mask2, ddf)

    fixed_mask1 = fixed_mask1.squeeze(0).squeeze(0)
    pred_mask1 = pred_mask1.squeeze(0).squeeze(0)
    fixed_mask1 = fixed_mask1.detach().to("cpu").numpy()
    pred_mask1 = pred_mask1.detach().to("cpu").numpy()

    fixed_mask2 = fixed_mask2.squeeze(0).squeeze(0)
    pred_mask2 = pred_mask2.squeeze(0).squeeze(0)
    fixed_mask2 = fixed_mask2.detach().to("cpu").numpy()
    pred_mask2 = pred_mask2.detach().to("cpu").numpy()

    pred_mask1[pred_mask1 > 0.5] = 1
    pred_mask1[pred_mask1 <= 0.5] = 0
    pred_mask2[pred_mask2 > 0.5] = 1
    pred_mask2[pred_mask2 <= 0.5] = 0

    tab1.append(function(fixed_mask1, pred_mask1))
    tab2.append(function(fixed_mask2, pred_mask2))

  return tab1, tab2    


# Model init
model = RegModel()
device = torch.device("cuda:0")
model = model.to(device)

# Cost function
criterion = nn.BCEWithLogitsLoss() 

# Optimizer and scheduler 
optimizer = torch.optim.Adam(model.parameters(), lr = model_params['learning_rate'], betas = model_params['betas']) ## lr
scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.995**epoch)

training_size = len(training_loader.dataset)
val_size = len(val_loader.dataset)

# Lists of train and val losses
loss_history = []
val_loss_history = []

best_loss = float("inf")
writer = tb.SummaryWriter(str(config['runs']) + '/Registration')

loss_history = []
regu_history = []
mind_history = []
val_loss_history = []
val_regu_history = []
val_mind_history = []


# Trainloop
for epoch in range(model_params['epochs_reg']):
  train_loss = 0
  r_loss = 0
  m_loss = 0
  val_loss = 0
  r_loss_val = 0
  m_loss_val = 0
  for batch in tqdm(training_loader):
    fixed = batch['imagef'][tio.DATA].to(device)
    moving = batch['imagem'][tio.DATA].to(device)


    optimizer.zero_grad()

    ddf = model(torch.cat((moving, fixed), dim=1)).permute(0,2,3,4,1)     
    pred_image = df_field_v2(moving, ddf)

    lambda_reg = model_params['lambda']

    first = fixed.shape[0]

    mind = mind_loss(pred_image, fixed)
    regu_loss = diffusion(ddf)
    loss = mind + lambda_reg*regu_loss

    mind += first*mind.item()
    regu_loss += first*regu_loss.item()

    train_loss += first*loss.item()

    loss.backward()
    optimizer.step()
  
  scheduler.step()

  train_loss /= training_size
  mind /= training_size
  regu_loss /= training_size


  print('Train Loss', train_loss)
  print('MIND Loss', mind)
  print('REGU Loss', regu_loss)
  loss_history.append(train_loss)
  mind_history.append(mind)
  regu_history.append(regu_loss)


  # Tensorboard train
  writer.add_scalars(f'MIND/tio{model_params['lambda']}', {
              'MIND': mind,
              }, global_step = epoch)

  writer.add_scalars(f'REGU/tio{model_params['lambda']}', {
              'REGU': regu_loss,
              }, global_step = epoch)

  writer.add_scalars(f'LOSS/tio{model_params['lambda']}', {
              'TRAIN LOSS': train_loss,
              }, global_step = epoch)
  

  # Validation
  for batch in tqdm(val_loader):
    fixed_image = batch['imagef'][tio.DATA].to(device)
    moving_image = batch['imagem'][tio.DATA].to(device)

    ddf = model(torch.cat((moving_image, fixed_image), dim=1)).permute(0,2,3,4,1)     

    pred_image = df_field_v2(moving_image, ddf)

    first = fixed_image.shape[0]

    mind = mind_loss(pred_image, fixed_image)
    regu_loss = diffusion(ddf)
    loss = mind + lambda_reg*regu_loss

    m_loss_val += first*mind.item()
    r_loss_val += first*regu_loss.item()

    val_loss += first*loss.item()

  val_loss /= val_size
  m_loss_val /= val_size
  r_loss_val /= val_size

  print('\nVal Loss', val_loss)
  print('MIND val Loss', m_loss_val)
  print('REGU val Loss', r_loss_val)


  val_loss_history.append(val_loss)
  val_regu_history.append(r_loss_val)
  val_mind_history.append(m_loss_val)

  # Tensorboard val
  writer.add_scalars(f'mind_val/tio{model_params['lambda']}', {
              'MIND VAL': m_loss_val,
              }, global_step = epoch)

  writer.add_scalars(f'regu_val/tio{model_params['lambda']}', {
              'REGU VAL': r_loss_val,
              }, global_step = epoch)

  writer.add_scalars(f'loss_val/tio{model_params['lambda']}', {
              'VAL LOSS': val_loss,
              }, global_step = epoch)



  # Counting Dice Score every n epochs 
  if (epoch+1)% model_params['epoch_save'] == 0:
      model.eval()
      with torch.no_grad():
          dc1, dc2 = count(model, val_loader, dc)
          std1 = np.std(dc1)
          av1 = sum(dc1)/len(dc1)
          std2 = np.std(dc2)
          av2 = sum(dc2)/len(dc2)

      model.train()
      print('\nDC after ' + str(epoch+1) + ' epochs: ' + str(av1) + ' +/- ' + str(std1))
      print('DC after ' + str(epoch+1) + ' epochs: ' + str(av2) + ' +/- ' + str(std2))
      print('\n-------------------------------------------------------')

  
  if (epoch+1) % model_params['epoch_save_reg'] == 0:
      path = f"/path/after_{epoch+1}.pt"
      torch.save({
          'model_state_dict': model.state_dict(),
          'optimizer_state_dict': optimizer.state_dict(),
          'train_loss': loss_history,
          'regu_train_loss': regu_history,
          'mind_train_loss': mind_history,
          'val_loss': val_loss_history,
          'regu_val_loss': val_regu_history,
          'mind_val_loss': val_mind_history,
      }, path)